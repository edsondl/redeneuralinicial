{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNZ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "urKhiP5_NG55"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() # Define a convenção de imagens para tensor\n",
        "trainset = datasets.MNIST('./MNIST_data', download=True,  train=True, transform=transform) # Carrega a parte de treino do datase\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform) # Carrega a parte de validação\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria um buffer par apegar os dados "
      ],
      "metadata": {
        "id": "oNpoBZ-ZPH0d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QTUuwI_rfSvb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import imagenet\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "BusvuOH6TkxI",
        "outputId": "a0d96dbf-8123-4c2e-eea9-fd98c83260ac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) # Para verificar as dimentsões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) # Para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFlsqo6SU8jS",
        "outputId": "e4225ea3-3e8b-4784-ed48-bd8ecc0f528a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # Camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # Camada de interna 1, 128 neurônios que se ligam a 65\n",
        "    self.linear3 = nn.Linear(64, 10) # Camada de interna 2, 64 neurônios que se ligam a 10\n",
        "    # Para a camada de saída não é necessário definir nada, pois, só precisamos o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) # Função de ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) # Função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) # Função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
        "    return F.log_softmax(X, dim=1) # Dados utilizados para calcular a perda \n"
      ],
      "metadata": {
        "id": "YSLPEfwTYVBb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def treino(modelo, trainloader, device):\n",
        "\n",
        "    otimizador = optim.S60(modelo.parameters(), lr=0.01, momentum=0.5) # Define a política de ativação dos pesos e da bias\n",
        "    inicio = time() # Timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "    criterio = nn.NLLLoss() # Definindo o critério para calcular a perda\n",
        "    EPOCHS = 10 # Número de epochs que o algoritmo rodará\n",
        "    modelo.train # Ativando o modo de treinamento do modelo\n",
        "    for epoch in range(EPOCHS):\n",
        "        perda_acumulada = 0 # Inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "        for imagens, etiquetas in trainloader:\n",
        "          imagens = imagens.view(imagens.shape[0], -1) # Convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis com\n",
        "          otimizador.zero_grand() # Zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "          output = modelo(imagens.to(device)) # Coloando os dados no modelo\n",
        "          perda_instantanea = criterio(output, etiquetas.to(device)) # Calculando a perda do epoch em questão\n",
        "\n",
        "          perda_instantanea.backward() # Back propagation a partir da perda\n",
        "\n",
        "          otimizador.step() # Atualizando os pesos e a bias\n",
        "\n",
        "          perda_acumulada += perda_instantanea.item() # Atualização da perda acumulada\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "    print(\"\\Tempo de treino (em minutos) = \", (time()-inicio)/60)       \n"
      ],
      "metadata": {
        "id": "ZP4pbE-5fXaJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas=0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # Desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # Output do modelo em escala logaritma\n",
        "\n",
        "        ps = torch.exp(logps) # Converte output para escala normal (lembrando que é um tensor)\n",
        "        probab = list (ps.cpu().numpy()[0])\n",
        "        etiqueta_pred = probab.index(max(probab)) # Converte o tensor em número, no caso, o número que o modelo previu\n",
        "        etiqueta_certa = etiquetas.numpy()[1]\n",
        "        if(etiqueta_certa==etiqueta_pred): # Compara a previsão com o valor correto\n",
        "          conta_corretas=1\n",
        "        conta_todas +=1\n",
        "  print(\"Total de imagens testadas = \", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\",format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "bcruEwzTrNQ3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuVV4XN9wVYk",
        "outputId": "9c427a9a-8cf6-4423-a404-814b5837965c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}